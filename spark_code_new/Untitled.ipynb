{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7588c87c-5982-437d-a132-8e8a28cbfd97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Imports for TF-IDF \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT, DenseVector\n",
    "from pyspark.sql.functions import concat_ws, collect_list, udf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc31df58-91f4-484e-a11a-3f770a9c83a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to create spark sessions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/17 01:57:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to print directory\n",
      "['.ipynb_checkpoints', 'Dockerfile', 'process_data.py', 'TF_IDF.ipynb', 'Untitled.ipynb']\n",
      "about to load data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+------+----------------+------------------+----------------------+------------------+--------+--------------------+--------------------+----+------+-------------+-------------+---------+------------+--------------------+--------------------+--------------------+----------+\n",
      "|             account|         application|                card|             content|          created_at|edited_at|emojis|favourites_count|                id|in_reply_to_account_id|    in_reply_to_id|language|   media_attachments|            mentions|poll|reblog|reblogs_count|replies_count|sensitive|spoiler_text|                tags|                 uri|                 url|visibility|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+------+----------------+------------------+----------------------+------------------+--------+--------------------+--------------------+----+------+-------------+-------------+---------+------------+--------------------+--------------------+--------------------+----------+\n",
      "|{eledoster@mamot....|                null|                null|<p>JEAN-LUC M√âLEN...|2023-05-11T20:50:...|     null|    []|               0|110352022464623595|                  null|              null|      fr|                  []|                  []|null|  null|            0|            0|    false|            |                  []|https://mamot.fr/...|https://mamot.fr/...|    public|\n",
      "|{riadsb, https://...| {RI_Aircraft, null}|{, , U4C@1[|=%2t8...|<p><a href=\"https...|2023-05-11T20:50:...|     null|    []|               0|110352022430247836|                  null|              null|      en|[{U3Dm2F000LDi%2%...|                  []|null|  null|            0|            0|    false|            |[{kbos, https://m...|https://mastodon....|https://mastodon....|    public|\n",
      "|{LVAces@sportsbot...|                null|                null|<p>TMC üèÅ</p><p><...|2023-05-11T20:49:...|     null|    []|               0|110352022430034382|                  null|              null|     und|[{UsL4sbxutRtR00a...|                  []|null|  null|            0|            0|    false|            |[{allin, https://...|https://sportsbot...|https://twitter.c...|    public|\n",
      "|{Chiefs@sportsbot...|                null|                null|<p>Don‚Äôt worry, w...|2023-05-11T20:50:...|     null|    []|               0|110352022359300568|                  null|              null|      en|[{UHEL8J0|Ip$%yZw...|                  []|null|  null|            0|            0|    false|            |                  []|https://sportsbot...|https://twitter.c...|    public|\n",
      "|{wdwstats, https:...|{WDWStats, https:...|{, , null, Actual...|<p>Spaceship Eart...|2023-05-11T20:50:...|     null|    []|               0|110352022307842240|    109852353549163881|110351892795929977|      nl|                  []|                  []|null|  null|            0|            0|    false|            |[{epcot, https://...|https://mastodon....|https://mastodon....|    public|\n",
      "|{Rayados@sportsbo...|                null|                null|<p>¬°Venga, Joao!?...|2023-05-11T20:50:...|     null|    []|               0|110352022304056883|                  null|              null|      et|[{UVE3oBWB.Tt7tRs...|                  []|null|  null|            0|            0|    false|            |                  []|https://sportsbot...|https://twitter.c...|    public|\n",
      "|{wdwstats, https:...|{WDWStats, https:...|{, , null, Actual...|<p>TriceraTop Spi...|2023-05-11T20:50:...|     null|    []|               0|110352022290952242|                  null|              null|      nl|                  []|                  []|null|  null|            0|            0|    false|            |[{animalkingdom, ...|https://mastodon....|https://mastodon....|    public|\n",
      "|{alex@social.nah....|                null|                null|                    |2023-05-11T20:50:...|     null|    []|               0|110352022280515082|                  null|              null|      fr|[{U3AAaa?b4nof%MW...|                  []|null|  null|            0|            0|    false|            |                  []|https://social.na...|https://social.na...|    public|\n",
      "|{thomholwerda@soc...|                null|                null|<p>lol windows on...|2023-05-11T20:50:...|     null|    []|               0|110352022238440493|                  null|              null|      en|                  []|                  []|null|  null|            0|            0|    false|            |                  []|https://social.tc...|https://social.tc...|    public|\n",
      "|{CheComunitario, ...|         {Web, null}|                null|<p>cof foco faro ...|2023-05-11T20:50:...|     null|    []|               0|110352022236906261|                  null|              null|      en|[{U0T9L#_3j[_3~qj...|                  []|null|  null|            0|            0|    false|            |                  []|https://mastodon....|https://mastodon....|    public|\n",
      "|{futpicante@sport...|                null|                null|<p>üèÜüëÄ Chivas as...|2023-05-11T20:49:...|     null|    []|               0|110352022189776959|                  null|              null|      es|                  []|                  []|null|  null|            0|            0|    false|            |                  []|https://sportsbot...|https://twitter.c...|    public|\n",
      "|{uxmark@mstdn.ca,...|                null|                null|<p>And finally, h...|2023-05-11T20:50:...|     null|    []|               0|110352022175095528|    109383325293248153|110352005258755939|      en|[{UEI;Fx@v=E.7?Hs...|                  []|null|  null|            0|            0|    false|            |[{biketooter, htt...|https://mstdn.ca/...|https://mstdn.ca/...|    public|\n",
      "|{fantomaus@nrw.so...|                null|                null|<p>That is <a hre...|2023-05-11T20:50:...|     null|    []|               0|110352022163247470|                  null|              null|      de|[{UWGj2z+.aj-h5+T...|                  []|null|  null|            0|            0|    false|            |[{Eurovision2023,...|https://nrw.socia...|https://nrw.socia...|    public|\n",
      "|{paulwinkler@mstd...|                null|                null|<p><a href=\"https...|2023-05-11T20:50:...|     null|    []|               0|110352022154524662|                  null|              null|      en|[{U68W?j0K0M^%9v%...|                  []|null|  null|            0|            0|    false|            |[{andrewlloydwebb...|https://mstdn.soc...|https://mstdn.soc...|    public|\n",
      "|{oops@blorst.xyz,...|                null|                null|<p>Entertainment ...|2023-05-11T20:50:...|     null|    []|               0|110352022154195152|                  null|              null|      en|[{UbMsp24n%hi^O[I...|                  []|null|  null|            0|            0|    false|            |                  []|https://blorst.xy...|https://blorst.xy...|    public|\n",
      "|{kensanata@tablet...|                null|                null|<p>Our Internet O...|2023-05-11T20:50:...|     null|    []|               0|110352022116073080|                  null|              null|      en|                  []|[{glenatron@dice....|null|  null|            0|            0|    false|            |                  []|https://tabletop....|https://tabletop....|    public|\n",
      "|{heiseonline@soci...|                null|                null|<p>Elon Musk k√ºnd...|2023-05-11T20:50:...|     null|    []|               0|110352022032192267|                  null|              null|      de|[{U+DLg3VYT}t7W?X...|                  []|null|  null|            0|            0|    false|            |[{elonmusk, https...|https://social.he...|https://social.he...|    public|\n",
      "|{sternecker@infos...|                null|                null|<p>Repeatedly put...|2023-05-11T20:50:...|     null|    []|               0|110352022018141420|                  null|              null|      en|                  []|                  []|null|  null|            0|            0|    false|            |                  []|https://infosec.e...|https://infosec.e...|    public|\n",
      "|{mahara@mastodon....|                null|                null|<p>The <a href=\"h...|2023-05-11T20:50:...|     null|    []|               0|110352022009872042|                  null|              null|      en|[{UNQv:hf6otaz0Jf...|                  []|null|  null|            0|            0|    false|            |[{portfolio, http...|https://mastodon....|https://mastodon....|    public|\n",
      "|{synkr3tyk, https...|{Mastodon for iOS...|                null|<p>This boi is c ...|2023-05-11T20:50:...|     null|    []|               0|110352021997198105|                  null|              null|      en|[{UXC6x*IVM|af~pM...|                  []|null|  null|            0|            0|    false|            |                  []|https://mastodon....|https://mastodon....|    public|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+------+----------------+------------------+----------------------+------------------+--------+--------------------+--------------------+----+------+-------------+-------------+---------+------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "loaded data correctly\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "# def main():\n",
    "\n",
    "# Initialize SparkSession\n",
    "print(\"about to create spark sessions\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"test-td-idf\")\\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .config(\"spark.executor.instances\", 1)\\\n",
    "    .config(\"spark.cores.max\", 2)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data from JSON file\n",
    "import os\n",
    "print(\"about to print directory\")\n",
    "print(os.listdir())\n",
    "\n",
    "print(\"about to load data\")\n",
    "data = spark.read.json(\"/opt/data/*.json\")\n",
    "print(\"read file\")\n",
    "data.show()\n",
    "print(\"loaded data correctly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a88efe39-1756-4774-a897-a47dcca31e39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account: struct (nullable = true)\n",
      " |    |-- acct: string (nullable = true)\n",
      " |    |-- avatar: string (nullable = true)\n",
      " |    |-- avatar_static: string (nullable = true)\n",
      " |    |-- bot: boolean (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- discoverable: boolean (nullable = true)\n",
      " |    |-- display_name: string (nullable = true)\n",
      " |    |-- emojis: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- shortcode: string (nullable = true)\n",
      " |    |    |    |-- static_url: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- visible_in_picker: boolean (nullable = true)\n",
      " |    |-- fields: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |    |-- verified_at: string (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- following_count: long (nullable = true)\n",
      " |    |-- group: boolean (nullable = true)\n",
      " |    |-- header: string (nullable = true)\n",
      " |    |-- header_static: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- last_status_at: string (nullable = true)\n",
      " |    |-- locked: boolean (nullable = true)\n",
      " |    |-- noindex: boolean (nullable = true)\n",
      " |    |-- note: string (nullable = true)\n",
      " |    |-- roles: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- username: string (nullable = true)\n",
      " |-- application: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- website: string (nullable = true)\n",
      " |-- card: struct (nullable = true)\n",
      " |    |-- author_name: string (nullable = true)\n",
      " |    |-- author_url: string (nullable = true)\n",
      " |    |-- blurhash: string (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- embed_url: string (nullable = true)\n",
      " |    |-- height: long (nullable = true)\n",
      " |    |-- html: string (nullable = true)\n",
      " |    |-- image: string (nullable = true)\n",
      " |    |-- language: string (nullable = true)\n",
      " |    |-- provider_name: string (nullable = true)\n",
      " |    |-- provider_url: string (nullable = true)\n",
      " |    |-- title: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- width: long (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- edited_at: string (nullable = true)\n",
      " |-- emojis: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- shortcode: string (nullable = true)\n",
      " |    |    |-- static_url: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- visible_in_picker: boolean (nullable = true)\n",
      " |-- favourites_count: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- in_reply_to_account_id: string (nullable = true)\n",
      " |-- in_reply_to_id: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- media_attachments: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- blurhash: string (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- meta: struct (nullable = true)\n",
      " |    |    |    |-- focus: struct (nullable = true)\n",
      " |    |    |    |    |-- x: double (nullable = true)\n",
      " |    |    |    |    |-- y: double (nullable = true)\n",
      " |    |    |    |-- original: struct (nullable = true)\n",
      " |    |    |    |    |-- aspect: double (nullable = true)\n",
      " |    |    |    |    |-- bitrate: long (nullable = true)\n",
      " |    |    |    |    |-- duration: double (nullable = true)\n",
      " |    |    |    |    |-- frame_rate: string (nullable = true)\n",
      " |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |-- size: string (nullable = true)\n",
      " |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |-- small: struct (nullable = true)\n",
      " |    |    |    |    |-- aspect: double (nullable = true)\n",
      " |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |-- size: string (nullable = true)\n",
      " |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |-- preview_remote_url: string (nullable = true)\n",
      " |    |    |-- preview_url: string (nullable = true)\n",
      " |    |    |-- remote_url: string (nullable = true)\n",
      " |    |    |-- text_url: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |-- mentions: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- acct: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      " |-- poll: struct (nullable = true)\n",
      " |    |-- emojis: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- expired: boolean (nullable = true)\n",
      " |    |-- expires_at: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- multiple: boolean (nullable = true)\n",
      " |    |-- options: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- title: string (nullable = true)\n",
      " |    |    |    |-- votes_count: long (nullable = true)\n",
      " |    |-- voters_count: long (nullable = true)\n",
      " |    |-- votes_count: long (nullable = true)\n",
      " |-- reblog: string (nullable = true)\n",
      " |-- reblogs_count: long (nullable = true)\n",
      " |-- replies_count: long (nullable = true)\n",
      " |-- sensitive: boolean (nullable = true)\n",
      " |-- spoiler_text: string (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |-- uri: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- visibility: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7881bca4-8444-48f2-a562-c1db5682114d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "# Remove html tags\n",
    "data = data.withColumn(\"content\", regexp_replace(\"content\", \"<[^>]*>\", \"\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9cec29-913a-4203-924e-0b9825457909",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# type(data.select('account').show(1))\n",
    "# data.select('account').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27d8de-e5b5-4415-bc9f-e2d12a248be0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c44ea536-74bb-4810-a2a3-75b404d9a6e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+------+----------------+------------------+----------------------+------------------+--------+--------------------+--------------------+----+------+-------------+-------------+---------+------------+--------------------+--------------------+--------------------+----------+\n",
      "|             account|         application|                card|             content|          created_at|edited_at|emojis|favourites_count|                id|in_reply_to_account_id|    in_reply_to_id|language|   media_attachments|            mentions|poll|reblog|reblogs_count|replies_count|sensitive|spoiler_text|                tags|                 uri|                 url|visibility|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+------+----------------+------------------+----------------------+------------------+--------+--------------------+--------------------+----+------+-------------+-------------+---------+------------+--------------------+--------------------+--------------------+----------+\n",
      "|{eledoster@mamot....|                null|                null|JEAN-LUC M√âLENCHO...|2023-05-11T20:50:...|     null|    []|               0|110352022464623595|                  null|              null|      fr|                  []|                  []|null|  null|            0|            0|    false|            |                  []|https://mamot.fr/...|https://mamot.fr/...|    public|\n",
      "|{riadsb, https://...| {RI_Aircraft, null}|{, , U4C@1[|=%2t8...|#KBOS / #KPVD - #...|2023-05-11T20:50:...|     null|    []|               0|110352022430247836|                  null|              null|      en|[{U3Dm2F000LDi%2%...|                  []|null|  null|            0|            0|    false|            |[{kbos, https://m...|https://mastodon....|https://mastodon....|    public|\n",
      "|{LVAces@sportsbot...|                null|                null|TMC üèÅ@_ajawilson...|2023-05-11T20:49:...|     null|    []|               0|110352022430034382|                  null|              null|     und|[{UsL4sbxutRtR00a...|                  []|null|  null|            0|            0|    false|            |[{allin, https://...|https://sportsbot...|https://twitter.c...|    public|\n",
      "|{Chiefs@sportsbot...|                null|                null|Don‚Äôt worry, we‚Äôv...|2023-05-11T20:50:...|     null|    []|               0|110352022359300568|                  null|              null|      en|[{UHEL8J0|Ip$%yZw...|                  []|null|  null|            0|            0|    false|            |                  []|https://sportsbot...|https://twitter.c...|    public|\n",
      "|{wdwstats, https:...|{WDWStats, https:...|{, , null, Actual...|Spaceship Earth h...|2023-05-11T20:50:...|     null|    []|               0|110352022307842240|    109852353549163881|110351892795929977|      nl|                  []|                  []|null|  null|            0|            0|    false|            |[{epcot, https://...|https://mastodon....|https://mastodon....|    public|\n",
      "|{Rayados@sportsbo...|                null|                null| ¬°Venga, Joao!üá™üá®üî•|2023-05-11T20:50:...|     null|    []|               0|110352022304056883|                  null|              null|      et|[{UVE3oBWB.Tt7tRs...|                  []|null|  null|            0|            0|    false|            |                  []|https://sportsbot...|https://twitter.c...|    public|\n",
      "|{wdwstats, https:...|{WDWStats, https:...|{, , null, Actual...|TriceraTop Spin h...|2023-05-11T20:50:...|     null|    []|               0|110352022290952242|                  null|              null|      nl|                  []|                  []|null|  null|            0|            0|    false|            |[{animalkingdom, ...|https://mastodon....|https://mastodon....|    public|\n",
      "|{alex@social.nah....|                null|                null|                    |2023-05-11T20:50:...|     null|    []|               0|110352022280515082|                  null|              null|      fr|[{U3AAaa?b4nof%MW...|                  []|null|  null|            0|            0|    false|            |                  []|https://social.na...|https://social.na...|    public|\n",
      "|{thomholwerda@soc...|                null|                null|lol windows on a ...|2023-05-11T20:50:...|     null|    []|               0|110352022238440493|                  null|              null|      en|                  []|                  []|null|  null|            0|            0|    false|            |                  []|https://social.tc...|https://social.tc...|    public|\n",
      "|{CheComunitario, ...|         {Web, null}|                null|cof foco faro mar...|2023-05-11T20:50:...|     null|    []|               0|110352022236906261|                  null|              null|      en|[{U0T9L#_3j[_3~qj...|                  []|null|  null|            0|            0|    false|            |                  []|https://mastodon....|https://mastodon....|    public|\n",
      "|{futpicante@sport...|                null|                null|üèÜüëÄ Chivas aspir...|2023-05-11T20:49:...|     null|    []|               0|110352022189776959|                  null|              null|      es|                  []|                  []|null|  null|            0|            0|    false|            |                  []|https://sportsbot...|https://twitter.c...|    public|\n",
      "|{uxmark@mstdn.ca,...|                null|                null|And finally, here...|2023-05-11T20:50:...|     null|    []|               0|110352022175095528|    109383325293248153|110352005258755939|      en|[{UEI;Fx@v=E.7?Hs...|                  []|null|  null|            0|            0|    false|            |[{biketooter, htt...|https://mstdn.ca/...|https://mstdn.ca/...|    public|\n",
      "|{fantomaus@nrw.so...|                null|                null|That is #Eurovisi...|2023-05-11T20:50:...|     null|    []|               0|110352022163247470|                  null|              null|      de|[{UWGj2z+.aj-h5+T...|                  []|null|  null|            0|            0|    false|            |[{Eurovision2023,...|https://nrw.socia...|https://nrw.socia...|    public|\n",
      "|{paulwinkler@mstd...|                null|                null|#BadCinderella wi...|2023-05-11T20:50:...|     null|    []|               0|110352022154524662|                  null|              null|      en|[{U68W?j0K0M^%9v%...|                  []|null|  null|            0|            0|    false|            |[{andrewlloydwebb...|https://mstdn.soc...|https://mstdn.soc...|    public|\n",
      "|{oops@blorst.xyz,...|                null|                null|Entertainment Ear...|2023-05-11T20:50:...|     null|    []|               0|110352022154195152|                  null|              null|      en|[{UbMsp24n%hi^O[I...|                  []|null|  null|            0|            0|    false|            |                  []|https://blorst.xy...|https://blorst.xy...|    public|\n",
      "|{kensanata@tablet...|                null|                null|Our Internet Offi...|2023-05-11T20:50:...|     null|    []|               0|110352022116073080|                  null|              null|      en|                  []|[{glenatron@dice....|null|  null|            0|            0|    false|            |                  []|https://tabletop....|https://tabletop....|    public|\n",
      "|{heiseonline@soci...|                null|                null|Elon Musk k√ºndigt...|2023-05-11T20:50:...|     null|    []|               0|110352022032192267|                  null|              null|      de|[{U+DLg3VYT}t7W?X...|                  []|null|  null|            0|            0|    false|            |[{elonmusk, https...|https://social.he...|https://social.he...|    public|\n",
      "|{sternecker@infos...|                null|                null|Repeatedly put in...|2023-05-11T20:50:...|     null|    []|               0|110352022018141420|                  null|              null|      en|                  []|                  []|null|  null|            0|            0|    false|            |                  []|https://infosec.e...|https://infosec.e...|    public|\n",
      "|{mahara@mastodon....|                null|                null|The #portfolio le...|2023-05-11T20:50:...|     null|    []|               0|110352022009872042|                  null|              null|      en|[{UNQv:hf6otaz0Jf...|                  []|null|  null|            0|            0|    false|            |[{portfolio, http...|https://mastodon....|https://mastodon....|    public|\n",
      "|{synkr3tyk, https...|{Mastodon for iOS...|                null|This boi is c r a...|2023-05-11T20:50:...|     null|    []|               0|110352021997198105|                  null|              null|      en|[{UXC6x*IVM|af~pM...|                  []|null|  null|            0|            0|    false|            |                  []|https://mastodon....|https://mastodon....|    public|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+------+----------------+------------------+----------------------+------------------+--------+--------------------+--------------------+----+------+-------------+-------------+---------+------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "before group by\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             account|    combined_content|\n",
      "+--------------------+--------------------+\n",
      "|{20thCenturyFoxes...|Would you fuck my...|\n",
      "|{2blv7@queer.part...|Klingt nach 90er ...|\n",
      "|{435d0addfa045737...|Good Friday, whos...|\n",
      "|{718_louis@mstdn....|Whenever you're f...|\n",
      "|{82341f882b6eabcd...|curl https://nost...|\n",
      "|{AITA@botsin.spac...|AITA for calling ...|\n",
      "|{AirplayAccess, h...|Country music has...|\n",
      "|{Alexandros2112@m...|#Yankees fans get...|\n",
      "|{Ally_SMMiller@ma...|https://open.subs...|\n",
      "|{Arcana@akko.disq...|Rust users or as ...|\n",
      "|{ArnoldSchiller@m...|Nur, bitte...Ich ...|\n",
      "|{BennyOtt, https:...|Ob man zum Finale...|\n",
      "|{BinroHeretic@mst...|\"And the #knowled...|\n",
      "|{BloodyHell@thebl...|Wordle 694 5/6‚¨ú‚¨ú?...|\n",
      "|{Bossito@mastodon...|I love Filomena C...|\n",
      "|{BraveGods@mastod...|Because of me?I'm...|\n",
      "|{CORDIS_EU@respub...|RT @ProjectPerfor...|\n",
      "|{Captn_Steve@troe...|Colorfull #Dive #...|\n",
      "|{CatBuddha@mstdn....|                    |\n",
      "|{CheComunitario, ...|cof foco faro mar...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use concat_ws() to combine the array of strings into a single column\n",
    "data = data.withColumn(\"content\", concat_ws(\" \", \"content\"))\n",
    "data.show()\n",
    "print(\"before group by\")\n",
    "# Use groupBy() and concat_ws() to combine the strings for rows with the same ID\n",
    "data = data.groupBy(\"account\").agg(concat_ws(\" \", collect_list(\"content\")).alias(\"combined_content\"))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a760b5b8-c51d-43f0-8bf4-bf69723ce22b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Output column words already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Tokenize content column\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_content\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m data\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# return\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# import time\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#         main()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#         time.sleep(300)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py:398\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Output column words already exists."
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize content column\n",
    "tokenizer = Tokenizer(inputCol=\"combined_content\", outputCol=\"words\")\n",
    "data = tokenizer.transform(data)\n",
    "data.show()\n",
    "\n",
    "# return\n",
    "\n",
    "# import time\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     print(\"in the main file\")\n",
    "#     while True:\n",
    "#         print(\"in the while loop\")\n",
    "#         main()\n",
    "#         time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad2c8a84-087e-4524-84f4-689370bacc07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after tokenization\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column rawFeatures already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Compute Term Frequencies\u001b[39;00m\n\u001b[1;32m      3\u001b[0m hashingTF \u001b[38;5;241m=\u001b[39m HashingTF(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrawFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mhashingTF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m data\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py:398\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column rawFeatures already exists."
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"after tokenization\")\n",
    "# Compute Term Frequencies\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "data = hashingTF.transform(data)\n",
    "data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e811f38e-8419-4a2d-a44f-9e6361e4700d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after term frequencies\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column features already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Compute Inverse Document Frequencies\u001b[39;00m\n\u001b[1;32m      3\u001b[0m idf \u001b[38;5;241m=\u001b[39m IDF(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrawFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m idfModel \u001b[38;5;241m=\u001b[39m \u001b[43midf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m idfModel\u001b[38;5;241m.\u001b[39mtransform(data)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column features already exists."
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"after term frequencies\")\n",
    "# Compute Inverse Document Frequencies\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(data)\n",
    "data = idfModel.transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d48eca9-af62-4642-961d-792c3b9bc5e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after idfModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/17 02:16:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/05/17 02:17:37 WARN TaskSetManager: Lost task 0.0 in stage 29.0 (TID 44) (192.168.32.5 executor 3): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:85)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$1879/0x000000080166aa68.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$689/0x00000008011df220.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$541/0x00000008010a16f0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "23/05/17 02:17:40 ERROR TaskSchedulerImpl: Lost executor 3 on 192.168.32.5: Command exited with code 52\n",
      "23/05/17 02:17:40 WARN TaskSetManager: Lost task 0.0 in stage 26.3 (TID 41) (192.168.32.5 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "23/05/17 02:17:40 WARN TaskSetManager: Lost task 0.1 in stage 29.0 (TID 45) (192.168.32.5 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "23/05/17 02:17:45 WARN TaskSetManager: Lost task 0.1 in stage 26.3 (TID 46) (192.168.32.5 executor 4): FetchFailed(null, shuffleId=7, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 7 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      ")\n",
      "23/05/17 02:17:45 WARN TaskSetManager: Lost task 0.2 in stage 29.0 (TID 47) (192.168.32.5 executor 4): FetchFailed(null, shuffleId=8, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 8 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      ")\n",
      "23/05/17 02:17:45 ERROR FileFormatWriter: Aborting job c845e165-0343-49ef-aa01-0cc16168188b.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 26 (parquet at <unknown>:0) has failed the maximum allowable number of times: 4. Most recent failure reason:\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 7 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1961)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2978)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "23/05/17 02:17:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             account|    combined_content|               words|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|{20thCenturyFoxes...|Would you fuck my...|[would, you, fuck...|(262144,[19876,44...|[0.0,0.0,0.0,0.0,...|\n",
      "|{2blv7@queer.part...|Klingt nach 90er ...|[klingt, nach, 90...|(262144,[26497,10...|[0.0,0.0,0.0,0.0,...|\n",
      "|{435d0addfa045737...|Good Friday, whos...|[good, friday,, w...|(262144,[83997,10...|[0.0,0.0,0.0,0.0,...|\n",
      "|{718_louis@mstdn....|Whenever you're f...|[whenever, you're...|(262144,[3530,139...|[0.0,0.0,0.0,0.0,...|\n",
      "|{82341f882b6eabcd...|curl https://nost...|[curl, https://no...|(262144,[15873,46...|[0.0,0.0,0.0,0.0,...|\n",
      "|{AITA@botsin.spac...|AITA for calling ...|[aita, for, calli...|(262144,[9420,198...|[0.0,0.0,0.0,0.0,...|\n",
      "|{AirplayAccess, h...|Country music has...|[country, music, ...|(262144,[12472,21...|[0.0,0.0,0.0,0.0,...|\n",
      "|{Alexandros2112@m...|#Yankees fans get...|[#yankees, fans, ...|(262144,[5381,155...|[0.0,0.0,0.0,0.0,...|\n",
      "|{Ally_SMMiller@ma...|https://open.subs...|[https://open.sub...|(262144,[133275,2...|[0.0,0.0,0.0,0.0,...|\n",
      "|{Arcana@akko.disq...|Rust users or as ...|[rust, users, or,...|(262144,[19036,27...|[0.0,0.0,0.0,0.0,...|\n",
      "|{ArnoldSchiller@m...|Nur, bitte...Ich ...|[nur,, bitte...ic...|(262144,[20649,22...|[0.0,0.0,0.0,0.0,...|\n",
      "|{BennyOtt, https:...|Ob man zum Finale...|[ob, man, zum, fi...|(262144,[22241,35...|[0.0,0.0,0.0,0.0,...|\n",
      "|{BinroHeretic@mst...|\"And the #knowled...|[\"and, the, #know...|(262144,[27576,48...|[0.0,0.0,0.0,0.0,...|\n",
      "|{BloodyHell@thebl...|Wordle 694 5/6‚¨ú‚¨ú?...|[wordle, 694, 5/6...|(262144,[112355,1...|[0.0,0.0,0.0,0.0,...|\n",
      "|{Bossito@mastodon...|I love Filomena C...|[i, love, filomen...|(262144,[2455,190...|[0.0,0.0,0.0,0.0,...|\n",
      "|{BraveGods@mastod...|Because of me?I'm...|[because, of, me?...|(262144,[25599,77...|[0.0,0.0,0.0,0.0,...|\n",
      "|{CORDIS_EU@respub...|RT @ProjectPerfor...|[rt, @projectperf...|(262144,[4629,853...|[0.0,0.0,0.0,0.0,...|\n",
      "|{Captn_Steve@troe...|Colorfull #Dive #...|[colorfull, #dive...|(262144,[14266,19...|[0.0,0.0,0.0,0.0,...|\n",
      "|{CatBuddha@mstdn....|                    |                  []|(262144,[249180],...|[0.0,0.0,0.0,0.0,...|\n",
      "|{CheComunitario, ...|cof foco faro mar...|[cof, foco, faro,...|(262144,[27051,55...|[0.0,0.0,0.0,0.0,...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "After convert to dense vector\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"after idfModel\")\n",
    "# Convert sparse vectors to dense vectors\n",
    "\n",
    "data.show()\n",
    "print(\"After convert to dense vector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14b1d3ff-5dc3-4abf-ad83-9bf79326ffa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after drop combined content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/17 02:18:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             account|    combined_content|               words|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|{20thCenturyFoxes...|Would you fuck my...|[would, you, fuck...|(262144,[19876,44...|[0.0,0.0,0.0,0.0,...|\n",
      "|{2blv7@queer.part...|Klingt nach 90er ...|[klingt, nach, 90...|(262144,[26497,10...|[0.0,0.0,0.0,0.0,...|\n",
      "|{435d0addfa045737...|Good Friday, whos...|[good, friday,, w...|(262144,[83997,10...|[0.0,0.0,0.0,0.0,...|\n",
      "|{718_louis@mstdn....|Whenever you're f...|[whenever, you're...|(262144,[3530,139...|[0.0,0.0,0.0,0.0,...|\n",
      "|{82341f882b6eabcd...|curl https://nost...|[curl, https://no...|(262144,[15873,46...|[0.0,0.0,0.0,0.0,...|\n",
      "|{AITA@botsin.spac...|AITA for calling ...|[aita, for, calli...|(262144,[9420,198...|[0.0,0.0,0.0,0.0,...|\n",
      "|{AirplayAccess, h...|Country music has...|[country, music, ...|(262144,[12472,21...|[0.0,0.0,0.0,0.0,...|\n",
      "|{Alexandros2112@m...|#Yankees fans get...|[#yankees, fans, ...|(262144,[5381,155...|[0.0,0.0,0.0,0.0,...|\n",
      "|{Ally_SMMiller@ma...|https://open.subs...|[https://open.sub...|(262144,[133275,2...|[0.0,0.0,0.0,0.0,...|\n",
      "|{Arcana@akko.disq...|Rust users or as ...|[rust, users, or,...|(262144,[19036,27...|[0.0,0.0,0.0,0.0,...|\n",
      "|{ArnoldSchiller@m...|Nur, bitte...Ich ...|[nur,, bitte...ic...|(262144,[20649,22...|[0.0,0.0,0.0,0.0,...|\n",
      "|{BennyOtt, https:...|Ob man zum Finale...|[ob, man, zum, fi...|(262144,[22241,35...|[0.0,0.0,0.0,0.0,...|\n",
      "|{BinroHeretic@mst...|\"And the #knowled...|[\"and, the, #know...|(262144,[27576,48...|[0.0,0.0,0.0,0.0,...|\n",
      "|{BloodyHell@thebl...|Wordle 694 5/6‚¨ú‚¨ú?...|[wordle, 694, 5/6...|(262144,[112355,1...|[0.0,0.0,0.0,0.0,...|\n",
      "|{Bossito@mastodon...|I love Filomena C...|[i, love, filomen...|(262144,[2455,190...|[0.0,0.0,0.0,0.0,...|\n",
      "|{BraveGods@mastod...|Because of me?I'm...|[because, of, me?...|(262144,[25599,77...|[0.0,0.0,0.0,0.0,...|\n",
      "|{CORDIS_EU@respub...|RT @ProjectPerfor...|[rt, @projectperf...|(262144,[4629,853...|[0.0,0.0,0.0,0.0,...|\n",
      "|{Captn_Steve@troe...|Colorfull #Dive #...|[colorfull, #dive...|(262144,[14266,19...|[0.0,0.0,0.0,0.0,...|\n",
      "|{CatBuddha@mstdn....|                    |                  []|(262144,[249180],...|[0.0,0.0,0.0,0.0,...|\n",
      "|{CheComunitario, ...|cof foco faro mar...|[cof, foco, faro,...|(262144,[27051,55...|[0.0,0.0,0.0,0.0,...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"after drop combined content\")\n",
    "to_dense = lambda v: DenseVector(v.toArray()) if isinstance(v, SparseVector) else v\n",
    "to_dense_udf = udf(to_dense, VectorUDT())\n",
    "data = data.withColumn(\"features\", to_dense_udf(\"features\"))\n",
    "data.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8067a8-0e1f-4204-b450-8afccdc2c701",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after withColumn\n",
      "root\n",
      " |-- account: struct (nullable = true)\n",
      " |    |-- acct: string (nullable = true)\n",
      " |    |-- avatar: string (nullable = true)\n",
      " |    |-- avatar_static: string (nullable = true)\n",
      " |    |-- bot: boolean (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- discoverable: boolean (nullable = true)\n",
      " |    |-- display_name: string (nullable = true)\n",
      " |    |-- emojis: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- shortcode: string (nullable = true)\n",
      " |    |    |    |-- static_url: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- visible_in_picker: boolean (nullable = true)\n",
      " |    |-- fields: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |    |-- verified_at: string (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- following_count: long (nullable = true)\n",
      " |    |-- group: boolean (nullable = true)\n",
      " |    |-- header: string (nullable = true)\n",
      " |    |-- header_static: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- last_status_at: string (nullable = true)\n",
      " |    |-- locked: boolean (nullable = true)\n",
      " |    |-- noindex: boolean (nullable = true)\n",
      " |    |-- note: string (nullable = true)\n",
      " |    |-- roles: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- username: string (nullable = true)\n",
      " |-- combined_content: string (nullable = false)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/17 02:19:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             account|    combined_content|               words|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|{20thCenturyFoxes...|Would you fuck my...|[would, you, fuck...|(262144,[19876,44...|[0.0,0.0,0.0,0.0,...|\n",
      "|{2blv7@queer.part...|Klingt nach 90er ...|[klingt, nach, 90...|(262144,[26497,10...|[0.0,0.0,0.0,0.0,...|\n",
      "|{435d0addfa045737...|Good Friday, whos...|[good, friday,, w...|(262144,[83997,10...|[0.0,0.0,0.0,0.0,...|\n",
      "|{718_louis@mstdn....|Whenever you're f...|[whenever, you're...|(262144,[3530,139...|[0.0,0.0,0.0,0.0,...|\n",
      "|{82341f882b6eabcd...|curl https://nost...|[curl, https://no...|(262144,[15873,46...|[0.0,0.0,0.0,0.0,...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/17 02:19:14 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "23/05/17 02:23:51 WARN TaskSetManager: Lost task 0.0 in stage 38.0 (TID 59) (192.168.32.5 executor 4): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/opt/warehouse/tf_idf3.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Unknown Source)\n",
      "\tat java.base/java.util.ArrayList.trimToSize(Unknown Source)\n",
      "\tat net.razorvine.pickle.Unpickler.load_appends(Unpickler.java:727)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:232)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:95)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$1632/0x0000000801461f60.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$2020/0x0000000801688e68.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec$$Lambda$803/0x000000080128e550.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$792/0x000000080126f170.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\n",
      "23/05/17 02:23:51 ERROR TaskSchedulerImpl: Lost executor 4 on 192.168.32.5: Command exited with code 50\n",
      "23/05/17 02:23:56 WARN TaskSetManager: Lost task 0.2 in stage 38.0 (TID 61) (192.168.32.5 executor 5): FetchFailed(null, shuffleId=11, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 11 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      ")\n",
      "23/05/17 02:23:59 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "23/05/17 02:28:00 WARN TaskSetManager: Lost task 0.0 in stage 38.1 (TID 64) (192.168.32.5 executor 5): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/opt/warehouse/tf_idf3.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Unknown Source)\n",
      "\tat java.base/java.util.ArrayList.grow(Unknown Source)\n",
      "\tat java.base/java.util.ArrayList.addAll(Unknown Source)\n",
      "\tat net.razorvine.pickle.Unpickler.load_appends(Unpickler.java:726)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:232)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:95)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$1505/0x000000080140c818.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$1689/0x00000008015e05d8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec$$Lambda$688/0x00000008011de970.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$689/0x00000008011ded50.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\n",
      "23/05/17 02:28:00 ERROR TaskSchedulerImpl: Lost executor 5 on 192.168.32.5: Command exited with code 50\n",
      "23/05/17 02:28:00 WARN TaskSetManager: Lost task 0.1 in stage 38.1 (TID 65) (192.168.32.5 executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Command exited with code 50\n",
      "23/05/17 02:28:05 WARN TaskSetManager: Lost task 0.2 in stage 38.1 (TID 66) (192.168.32.5 executor 6): FetchFailed(null, shuffleId=11, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 11 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      ")\n",
      "23/05/17 02:28:07 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "23/05/17 02:32:05 WARN TaskSetManager: Lost task 0.0 in stage 38.2 (TID 69) (192.168.32.5 executor 6): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/opt/warehouse/tf_idf3.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Unknown Source)\n",
      "\tat java.base/java.util.ArrayList.grow(Unknown Source)\n",
      "\tat java.base/java.util.ArrayList.addAll(Unknown Source)\n",
      "\tat net.razorvine.pickle.Unpickler.load_appends(Unpickler.java:726)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:232)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:95)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$1524/0x00000008014083e0.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$1706/0x00000008015e17e8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec$$Lambda$688/0x00000008011de000.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$689/0x00000008011dbcf0.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\n",
      "23/05/17 02:32:05 ERROR TaskSchedulerImpl: Lost executor 6 on 192.168.32.5: Command exited with code 50\n",
      "23/05/17 02:32:05 WARN TaskSetManager: Lost task 0.1 in stage 38.2 (TID 70) (192.168.32.5 executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Command exited with code 50\n",
      "23/05/17 02:32:10 WARN TaskSetManager: Lost task 0.2 in stage 38.2 (TID 71) (192.168.32.5 executor 7): FetchFailed(null, shuffleId=11, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 11 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n",
      ")\n",
      "23/05/17 02:32:13 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "[Stage 38:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "print(\"after withColumn\")\n",
    "data.printSchema()\n",
    "data.show(5)\n",
    "# Write to file\n",
    "data.write.parquet(path=\"/opt/warehouse/tf_idf3.parquet\",mode=\"overwrite\")\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca0835f-7ed3-44d6-b17d-d106f8fa57a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "from pyspark.ml.linalg import VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b60af-c9f6-48ac-b95f-1e5b1660b3f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"account\", StringType(), True),\n",
    "    StructField(\"combined_content\", StringType(), True),\n",
    "    StructField(\"words\", ArrayType(StringType()), True),\n",
    "    StructField(\"rawFeatures\", VectorUDT(), True),\n",
    "    StructField(\"features\", VectorUDT(), True)\n",
    "])\n",
    "tfidf = spark.read.schema(schema).parquet('/opt/warehouse/tf_idf3.parquet')\n",
    "tfidf_df=tfidf.toPandas()\n",
    "tfidf_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c84afa-b451-469f-8167-b3dfa420f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_list = tfidf_df[['account']].to_dict('records')\n",
    "print(users_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4509e-0c2d-4634-a15f-80b3a016c604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf = spark.read.schema(schema).parquet('/opt/warehouse/tf_idf3.parquet')\n",
    "tfidf_df=tfidf.toPandas()\n",
    "users_list = tfidf_df[['account']].to_dict('records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
