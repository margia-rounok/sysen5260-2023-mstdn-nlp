{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be532e36-787a-49ce-9fb0-66f29c359c45",
   "metadata": {},
   "source": [
    "# Hello Spark\n",
    "Demonstration based on the [Spark Quick Start](https://spark.apache.org/docs/latest/quick-start.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d108d8-c350-4fb8-b183-8e853b3dc707",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import log10, explode\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d51df100-3506-495a-bfaf-00cda9fae8c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682639678.json  1682639688.json  1682639718.json  1682639934.json\n",
      "1682639681.json  1682639691.json  1682639745.json  1682640068.json\n",
      "1682639684.json  1682639696.json  1682639797.json  as-you-like-it.txt\n",
      "1682639686.json  1682639704.json  1682639858.json\n"
     ]
    }
   ],
   "source": [
    "ls  /opt/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb10a5-1b8d-4246-a12e-426ee81fff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e3d951-4ca3-463b-a472-a31c237cac21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/01 00:35:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/01 00:35:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"TF-IDF\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6e71734-7325-4179-90dd-fe02f24fd196",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load data from JSON file\n",
    "data = spark.read.json(\"../../../opt/data/1682639678.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "283c3c22-7371-4425-861e-a8ab7bf9f0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account: struct (nullable = true)\n",
      " |    |-- acct: string (nullable = true)\n",
      " |    |-- avatar: string (nullable = true)\n",
      " |    |-- avatar_static: string (nullable = true)\n",
      " |    |-- bot: boolean (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- discoverable: boolean (nullable = true)\n",
      " |    |-- display_name: string (nullable = true)\n",
      " |    |-- emojis: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- shortcode: string (nullable = true)\n",
      " |    |    |    |-- static_url: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- visible_in_picker: boolean (nullable = true)\n",
      " |    |-- fields: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |    |-- verified_at: string (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- following_count: long (nullable = true)\n",
      " |    |-- group: boolean (nullable = true)\n",
      " |    |-- header: string (nullable = true)\n",
      " |    |-- header_static: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- last_status_at: string (nullable = true)\n",
      " |    |-- locked: boolean (nullable = true)\n",
      " |    |-- noindex: boolean (nullable = true)\n",
      " |    |-- note: string (nullable = true)\n",
      " |    |-- roles: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- username: string (nullable = true)\n",
      " |-- application: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- website: string (nullable = true)\n",
      " |-- card: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- edited_at: string (nullable = true)\n",
      " |-- emojis: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- favourites_count: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- in_reply_to_account_id: string (nullable = true)\n",
      " |-- in_reply_to_id: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- media_attachments: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- blurhash: string (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- meta: struct (nullable = true)\n",
      " |    |    |    |-- original: struct (nullable = true)\n",
      " |    |    |    |    |-- aspect: double (nullable = true)\n",
      " |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |-- size: string (nullable = true)\n",
      " |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |-- small: struct (nullable = true)\n",
      " |    |    |    |    |-- aspect: double (nullable = true)\n",
      " |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |-- size: string (nullable = true)\n",
      " |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |-- preview_remote_url: string (nullable = true)\n",
      " |    |    |-- preview_url: string (nullable = true)\n",
      " |    |    |-- remote_url: string (nullable = true)\n",
      " |    |    |-- text_url: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |-- mentions: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- poll: string (nullable = true)\n",
      " |-- reblog: string (nullable = true)\n",
      " |-- reblogs_count: long (nullable = true)\n",
      " |-- replies_count: long (nullable = true)\n",
      " |-- sensitive: boolean (nullable = true)\n",
      " |-- spoiler_text: string (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |-- uri: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- visibility: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da54527-5d3f-4d70-9ed8-9c37d45f4ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------------------+\n",
      "|                id|                  combined_content|\n",
      "+------------------+----------------------------------+\n",
      "|110273473674788626|              <p>Meta is going ...|\n",
      "|110273473488790997|<p>ÈáëÂçÅ‰∏∫‰ªÄ‰πàÁªôÊàë‰∏ÄÁßçÊºÇÊµÅÊïôÂÆ§ÁöÑ...|\n",
      "|110273473835154582|              <p>Musk claims he...|\n",
      "|110273473375300613|              <p>April 27 7:52p...|\n",
      "|110273473319144564|              <p>$4 Pick Up. <a...|\n",
      "|110273473819204072|              <p>They‚Äôre ‚Äòskeet...|\n",
      "|110273473430816942|              <p>Gluten-free, s...|\n",
      "|110273473378589701|              <p>My energy is W...|\n",
      "|110273473636530708|<p>„Åæ„Åò„ÇÅ„Å´„Ç≥„ÉÑ„Ç≥„ÉÑ„ÇÑ„Å£„Å¶„Åç„Åü„Åã...|\n",
      "|110273473383805187|              <p>La sensaci√≥n d...|\n",
      "|110273473595453936|<p>ÊØèÁîªÂÆå‰∏ÄÂº†Á≠æÁªòÂ∞±Ë¢´‰∏ëÂæó„ÄÇ„ÄÇ„ÄÇ...|\n",
      "|110273473727702772|              <p>Los jueves son...|\n",
      "|110273473344515693|                 <p>„Åä„ÅØ„Åß„ÅôÔΩû</p>|\n",
      "|110273473299954485|      <p><span>„ÉØ„Ç§„ÇíÂ∞äÊï¨Ôºü„Åó„Å¶...|\n",
      "|110273473328750542|              <p>Eagles, Cardin...|\n",
      "|110273473850186596|              <p>üì∑ <a href=\"ht...|\n",
      "|110273473776725352|<p>ÈòøÂü∫ÁêâÊñØÂêåÊÑèÂú®ÁâπÊ¥õ‰ºä‰∫∫‰∏∫Ëµ´ÂÖã...|\n",
      "|110273473543772976|              <p>Allrighty, it'...|\n",
      "|110273473483464582|              <p>New post on th...|\n",
      "|110273473800528083|              <p>I said on Bird...|\n",
      "+------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import log10, concat_ws, flatten,collect_list\n",
    "\n",
    "# Use concat_ws() to combine the array of strings into a single column\n",
    "data = data.withColumn(\"content\", concat_ws(\" \", \"content\"))\n",
    "\n",
    "# Use groupBy() and concat_ws() to combine the strings for rows with the same ID\n",
    "data = data.groupBy(\"id\").agg(concat_ws(\" \", collect_list(\"content\")).alias(\"combined_content\"))\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "674a9a93-e1c4-40e0-9a50-f61ddb647c86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5984f640-f24a-4f47-988c-30a8868728a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "# from pyspark.ml.functions import to_dense_udf\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT, DenseVector\n",
    "from pyspark.sql.functions import concat_ws, collect_list, udf\n",
    "\n",
    "import numpy\n",
    "# Tokenize content column\n",
    "tokenizer = Tokenizer(inputCol=\"combined_content\", outputCol=\"words\")\n",
    "data = tokenizer.transform(data)\n",
    "\n",
    "# Compute Term Frequencies\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "data = hashingTF.transform(data)\n",
    "\n",
    "# Compute Inverse Document Frequencies\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(data)\n",
    "data = idfModel.transform(data)\n",
    "\n",
    "# Convert sparse vectors to dense vectors\n",
    "to_dense = lambda v: DenseVector(v.toArray()) if isinstance(v, SparseVector) else v\n",
    "to_dense_udf = udf(to_dense, VectorUDT())\n",
    "data = data.withColumn(\"features\", to_dense_udf(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6befd809-746e-440c-b8cb-b010a8c2e6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/01 00:35:51 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the result to a Parquet file\n",
    "data.write.parquet(\"/opt/warehouse/tf_idf3.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3ca7fd0-5e12-4c6b-89a1-50a98a1b3f39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------------------+---------------------------------+--------------------+--------------------+\n",
      "|                id|                  combined_content|                            words|         rawFeatures|            features|\n",
      "+------------------+----------------------------------+---------------------------------+--------------------+--------------------+\n",
      "|110273473674788626|              <p>Meta is going ...|             [<p>meta, is, goi...|(262144,[2348,706...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473488790997|<p>ÈáëÂçÅ‰∏∫‰ªÄ‰πàÁªôÊàë‰∏ÄÁßçÊºÇÊµÅÊïôÂÆ§ÁöÑ...|[<p>ÈáëÂçÅ‰∏∫‰ªÄ‰πàÁªôÊàë‰∏ÄÁßçÊºÇÊµÅÊïôÂÆ§...|(262144,[251637],...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473835154582|              <p>Musk claims he...|             [<p>musk, claims,...|(262144,[2348,568...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473375300613|              <p>April 27 7:52p...|             [<p>april, 27, 7:...|(262144,[2348,114...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473319144564|              <p>$4 Pick Up. <a...|             [<p>$4, pick, up....|(262144,[14352,38...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473819204072|              <p>They‚Äôre ‚Äòskeet...|             [<p>they‚Äôre, ‚Äòske...|(262144,[2348,134...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473430816942|              <p>Gluten-free, s...|             [<p>gluten-free,,...|(262144,[45638,12...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473378589701|              <p>My energy is W...|             [<p>my, energy, i...|(262144,[16185,64...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473636530708|<p>„Åæ„Åò„ÇÅ„Å´„Ç≥„ÉÑ„Ç≥„ÉÑ„ÇÑ„Å£„Å¶„Åç„Åü„Åã...|[<p>„Åæ„Åò„ÇÅ„Å´„Ç≥„ÉÑ„Ç≥„ÉÑ„ÇÑ„Å£„Å¶„Åç„Åü...|(262144,[161070],...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473383805187|              <p>La sensaci√≥n d...|             [<p>la, sensaci√≥n...|(262144,[2348,143...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473595453936|<p>ÊØèÁîªÂÆå‰∏ÄÂº†Á≠æÁªòÂ∞±Ë¢´‰∏ëÂæó„ÄÇ„ÄÇ„ÄÇ...|[<p>ÊØèÁîªÂÆå‰∏ÄÂº†Á≠æÁªòÂ∞±Ë¢´‰∏ëÂæó„ÄÇ„ÄÇ...|(262144,[78044],[...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473727702772|              <p>Los jueves son...|             [<p>los, jueves, ...|(262144,[2348,275...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473344515693|                 <p>„Åä„ÅØ„Åß„ÅôÔΩû</p>|              [<p>„Åä„ÅØ„Åß„ÅôÔΩû</p>]|(262144,[10890],[...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473299954485|      <p><span>„ÉØ„Ç§„ÇíÂ∞äÊï¨Ôºü„Åó„Å¶...|      [<p><span>„ÉØ„Ç§„ÇíÂ∞äÊï¨Ôºü„Åó...|(262144,[100502],...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473328750542|              <p>Eagles, Cardin...|             [<p>eagles,, card...|(262144,[2348,153...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473850186596|              <p>üì∑ <a href=\"ht...|             [<p>üì∑, <a, href=...|(262144,[7259,474...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473776725352|<p>ÈòøÂü∫ÁêâÊñØÂêåÊÑèÂú®ÁâπÊ¥õ‰ºä‰∫∫‰∏∫Ëµ´ÂÖã...|[<p>ÈòøÂü∫ÁêâÊñØÂêåÊÑèÂú®ÁâπÊ¥õ‰ºä‰∫∫‰∏∫Ëµ´...|(262144,[237437],...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473543772976|              <p>Allrighty, it'...|             [<p>allrighty,, i...|(262144,[2348,253...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473483464582|              <p>New post on th...|             [<p>new, post, on...|(262144,[2348,207...|[0.0,0.0,0.0,0.0,...|\n",
      "|110273473800528083|              <p>I said on Bird...|             [<p>i, said, on, ...|(262144,[1603,337...|[0.0,0.0,0.0,0.0,...|\n",
      "+------------------+----------------------------------+---------------------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf = spark.read.parquet('/opt/warehouse/tf_idf3.parquet/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2920c52-ab1c-4427-a8e4-6b2f7d14ab1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[0.0,0.0,0.0,0.0,...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf.select(\"features\").show(1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d650468f-5c10-4446-a71a-c7549f182873",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "first_element = tf_idf.select(\"features\").first()[0]\n",
    "print(first_element[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e7d9114-b4cb-4309-ba69-0de65b1f19d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<p>meta', 'is', 'going', 'to', 'let', 'you', 'update', 'quest', 'apps', 'before', 'the', 'headset', 'shuts', 'down', '<a', 'href=\"https://www.theverge.com/2023/4/27/23701286/meta-quest-v53-update-apps-headset-shuts-down\"', 'rel=\"nofollow', 'noopener', 'noreferrer\"', 'target=\"_blank\"><span', 'class=\"invisible\">https://www.</span><span', 'class=\"ellipsis\">theverge.com/2023/4/27/2370128</span><span', 'class=\"invisible\">6/meta-quest-v53-update-apps-headset-shuts-down</span></a></p>']\n"
     ]
    }
   ],
   "source": [
    "first_element = tf_idf.select(\"words\").first()[0]\n",
    "print(first_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b2158b-f29a-4583-9c50-97d4a39937bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98554c57-9ab7-4634-b64d-e9b2aca3edbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokens = data.selectExpr(\"id\", \"split(combined_content, ' ') as words\") \\\n",
    "#     .withColumn(\"word\", explode(\"words\")) \\\n",
    "#     .select(\"id\", \"word\")\n",
    "\n",
    "# tf = tokens.rdd.map(lambda x: ((x[0], x[1]), 1)) \\\n",
    "#     .reduceByKey(lambda x, y: x + y) \\\n",
    "#     .map(lambda x: (x[0][0], (x[0][1], x[1]))) \\\n",
    "#     .groupByKey() \\\n",
    "#     .mapValues(dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b72958f0-11c8-45b2-961f-cf5808e6d70a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "172bb289-48b6-4448-ba68-08de7463dbb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf_df = tf.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8dfad9fa-bb38-4717-a840-a7e9485b902b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# print(type(tf_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0ed81e7-636d-4a05-846a-1cfacc9c8582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|                id|                word|\n",
      "+------------------+--------------------+\n",
      "|110273473674788626|             <p>Meta|\n",
      "|110273473674788626|                  is|\n",
      "|110273473674788626|               going|\n",
      "|110273473674788626|                  to|\n",
      "|110273473674788626|                 let|\n",
      "|110273473674788626|                 you|\n",
      "|110273473674788626|              update|\n",
      "|110273473674788626|               Quest|\n",
      "|110273473674788626|                apps|\n",
      "|110273473674788626|              before|\n",
      "|110273473674788626|                 the|\n",
      "|110273473674788626|             headset|\n",
      "|110273473674788626|               shuts|\n",
      "|110273473674788626|                down|\n",
      "|110273473674788626|                  <a|\n",
      "|110273473674788626|href=\"https://www...|\n",
      "|110273473674788626|       rel=\"nofollow|\n",
      "|110273473674788626|            noopener|\n",
      "|110273473674788626|         noreferrer\"|\n",
      "|110273473674788626|target=\"_blank\"><...|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokens.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "890d4c54-9ecb-4fd8-b4e4-2a1fa5e56d53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Calculate inverse document frequency (IDF) for each word\n",
    "# idf = tokens.rdd.map(lambda x: (x[1], x[0])) \\\n",
    "#     .groupByKey() \\\n",
    "#     .map(lambda x: (x[0], log10(data.count()/len(x[1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4af1a1e7-886b-4f4c-95d1-cbbfd7f1c7e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "# print(type(idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dad2a422-a244-488e-994b-5373ce125338",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/pyspark/serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 462, in __getnewargs__\n",
      "    raise RuntimeError(\n",
      "RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/spark/python/pyspark/serializers.py:459\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py:632\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/context.py:462\u001b[0m, in \u001b[0;36mSparkContext.__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getnewargs__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# This method is called when attempting to pickle SparkContext, which is always an error:\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt appears that you are attempting to reference SparkContext from a broadcast \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable, action, or transformation. SparkContext can only be used on the driver, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot in code that it run on workers. For more information, see SPARK-5063.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43midf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:2836\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2833\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2835\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 2836\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2838\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   2839\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/context.py:2319\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2317\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   2318\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2319\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mrunJob(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), \u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m, partitions)\n\u001b[1;32m   2320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:5441\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5439\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 5441\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5442\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[1;32m   5443\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[1;32m   5447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[1;32m   5448\u001b[0m )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:5241\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   5239\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5240\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 5241\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5242\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSimplePythonFunction(\n\u001b[1;32m   5244\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[1;32m   5245\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5250\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[1;32m   5251\u001b[0m )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:5224\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   5221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext\u001b[39m\u001b[38;5;124m\"\u001b[39m, command: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[1;32m   5222\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   5223\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 5224\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5225\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   5227\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/serializers.py:469\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    467\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    468\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 469\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "# idf.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1085b508-a6fb-4f03-9ce7-45e7f1fdab38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/pyspark/serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 462, in __getnewargs__\n",
      "    raise RuntimeError(\n",
      "RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/spark/python/pyspark/serializers.py:459\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py:632\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/context.py:462\u001b[0m, in \u001b[0;36mSparkContext.__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getnewargs__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# This method is called when attempting to pickle SparkContext, which is always an error:\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt appears that you are attempting to reference SparkContext from a broadcast \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable, action, or transformation. SparkContext can only be used on the driver, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot in code that it run on workers. For more information, see SPARK-5063.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m idf_df \u001b[38;5;241m=\u001b[39m \u001b[43midf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:115\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    +---+\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRatio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1276\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1275\u001b[0m     )\n\u001b[0;32m-> 1276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1316\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[0;32m-> 1316\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1318\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:931\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 931\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    933\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:874\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inferSchema\u001b[39m(\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    855\u001b[0m     rdd: RDD[Any],\n\u001b[1;32m    856\u001b[0m     samplingRatio: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    857\u001b[0m     names: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StructType:\n\u001b[1;32m    859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m    Infer schema from an RDD of Row, dict, or tuple.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;124;03m    :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 874\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(first) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first row in RDD is empty, can not infer schema\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:2869\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2843\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   2844\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2845\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[1;32m   2846\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2867\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2869\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   2871\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:2836\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2833\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2835\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 2836\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2838\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   2839\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/context.py:2319\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2317\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   2318\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2319\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mrunJob(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), \u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m, partitions)\n\u001b[1;32m   2320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:5441\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5439\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 5441\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5442\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[1;32m   5443\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[1;32m   5447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[1;32m   5448\u001b[0m )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:5241\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   5239\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5240\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 5241\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5242\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSimplePythonFunction(\n\u001b[1;32m   5244\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[1;32m   5245\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5250\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[1;32m   5251\u001b[0m )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:5224\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   5221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext\u001b[39m\u001b[38;5;124m\"\u001b[39m, command: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[1;32m   5222\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   5223\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 5224\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5225\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   5227\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/serializers.py:469\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    467\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    468\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 469\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "idf_df = idf.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6fea7c-cb80-406b-a95a-93bfe223b0d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(type(idf_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "963e70c6-70a7-43d1-9d43-0e6c589b6e90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute '_jdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Multiply \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Multiply TF by IDF to get TF-IDF score\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tf_idf \u001b[38;5;241m=\u001b[39m \u001b[43mtf_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43midf\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;241m0\u001b[39m], {k: v\u001b[38;5;241m*\u001b[39mx[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m x[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()}))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:2337\u001b[0m, in \u001b[0;36mDataFrame.join\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   2334\u001b[0m         on \u001b[38;5;241m=\u001b[39m on\u001b[38;5;241m.\u001b[39m_jc\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m how \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2337\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m)\n\u001b[1;32m   2338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute '_jdf'"
     ]
    }
   ],
   "source": [
    "## Multiply \n",
    "# Multiply TF by IDF to get TF-IDF score\n",
    "tf_idf = tf_df.join(idf) \\\n",
    "    .map(lambda x: (x[0], {k: v*x[1][1] for k, v in x[1][0].items()}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72210e9d-5643-43d6-8655-1c9648cfce5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_idf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert TF-IDF scores to a DataFrame and store as Parquet file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tf_idf_df \u001b[38;5;241m=\u001b[39m \u001b[43mtf_idf\u001b[49m\u001b[38;5;241m.\u001b[39mtoDF([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_idf\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m tf_idf_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_idf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf_idf' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert TF-IDF scores to a DataFrame and store as Parquet file\n",
    "tf_idf_df = tf_idf.toDF([\"id\", \"tf_idf\"])\n",
    "tf_idf_df.write.mode(\"overwrite\").parquet(\"tf_idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b83b74-346c-48ce-b44a-0b3dd7dca92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23802ec5-981f-49c8-8bea-54a45beaa1f3",
   "metadata": {},
   "source": [
    "# Create a Spark Session\n",
    "The SparkSession object is our connection to the Spark Context Manager running on the spark-master host.\n",
    "\n",
    "There are a few important details in the setting up of the SparkSession:\n",
    "1. The `appName` is what shows up in the \"Running Apps\" section of http://localhost:8080/ -- It'll move to \"Completed Apps\" once we call `.stop()` on this session.\n",
    "2. The `master` tells it where to our Spark config-manager so we can launch spark-applications from this session.\n",
    "3. The `spark.sql.warehouse.dir` tells it where to find our Hive tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96de8a0-0b9e-4a3b-bc83-75ac4d1ee0af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd38c959-3f57-4456-83a1-e6e571ce1319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/30 23:05:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder\\\n",
    "    .appName(\"hello-pyspark\")\\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .config(\"spark.executor.instances\", 1)\\\n",
    "    .config(\"spark.cores.max\", 2)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d2f19-11dc-4d6f-8d85-6dc23b6751ad",
   "metadata": {},
   "source": [
    "# Word Count\n",
    "This is a very basic hello-world to make sure the we can run a little PySpark:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0230c4e-b8dc-4197-89e6-05b52cb1cef6",
   "metadata": {},
   "source": [
    "### Get Some Sample Data\n",
    "We pull Shakespeare's \"As You Like It\" from Project Gutenberg, and write it to `/opt/data`.  This is mounted to our `fileshare` volume which is mounted on this docker container as well as all of the spark-containers (master and worker(s)).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a25e3f3-5e3c-44bc-8a22-ca97896da3dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "resp = requests.get('https://www.gutenberg.org/cache/epub/1121/pg1121.txt')\n",
    "with open('/opt/data/as-you-like-it.txt','w')as fp:\n",
    "    fp.write(resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd8b419-3ea7-499b-8110-7e2acde23b13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682639678.json  1682639688.json  1682639718.json  1682639934.json\n",
      "1682639681.json  1682639691.json  1682639745.json  1682640068.json\n",
      "1682639684.json  1682639696.json  1682639797.json  as-you-like-it.txt\n",
      "1682639686.json  1682639704.json  1682639858.json\n"
     ]
    }
   ],
   "source": [
    "ls /opt/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844aaaa-7434-41dc-afae-e830d74e9848",
   "metadata": {},
   "source": [
    "### Perform word-count on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "510dc8d4-1653-4c61-99b8-913f3657a38a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ayli = spark_session.read.text('/opt/data/as-you-like-it.txt')\n",
    "ans = ayli.count()\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cdac81d-9644-48d2-84d8-4884fbaae6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/16 21:37:43 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "23/04/16 21:37:43 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:978)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "ayli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e96b5-b57b-43e9-8e67-fbb06b7ceaa7",
   "metadata": {},
   "source": [
    "# Spark grep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33b897d6-be9f-40ce-a4f7-b7d15bc732cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orlandos_lines = ayli.filter(ayli.value.contains(\"ORLANDO\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce6a5101-c556-41cd-91d4-6bbaf56404c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|  ORLANDO,  \"   \"...|\n",
      "|Enter ORLANDO and...|\n",
      "|  ORLANDO. As I r...|\n",
      "|  ORLANDO. Go apa...|\n",
      "|  ORLANDO. Nothin...|\n",
      "|  ORLANDO. Marry,...|\n",
      "|  ORLANDO. Shall ...|\n",
      "|  ORLANDO. O, sir...|\n",
      "|  ORLANDO. Ay, be...|\n",
      "|  ORLANDO. Come, ...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orlandos_lines.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a973ed6e-e9dd-48d3-8bed-f8cc8237cb67",
   "metadata": {},
   "source": [
    "# Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "426e9edd-b140-45a4-9d21-a10552c54e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "wordCounts = ayli.select(explode(split(ayli.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()\n",
    "_coll = wordCounts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6261ceb4-c443-4f2a-adfd-ff42fc131b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       word|count|\n",
      "+-----------+-----+\n",
      "|     online|    4|\n",
      "|PERMISSION.|    7|\n",
      "|       some|   26|\n",
      "|  disgrace,|    1|\n",
      "|       hope|    8|\n",
      "|      still|    7|\n",
      "|         By|   24|\n",
      "| misplaced;|    1|\n",
      "|      those|    8|\n",
      "|    knight,|    1|\n",
      "| FREDERICK.|   20|\n",
      "|  wrestler?|    1|\n",
      "|    embrace|    1|\n",
      "|        art|   21|\n",
      "|      burs,|    1|\n",
      "| likelihood|    1|\n",
      "|     travel|    3|\n",
      "|assailants.|    1|\n",
      "|      cold,|    1|\n",
      "|    blossom|    1|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordCounts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8796cdf1-e370-44d2-bbbd-75e0a2b9ecda",
   "metadata": {},
   "source": [
    "## Save as Parquet File\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39f6624c-c3dd-4c7d-9ce9-ecea5e4ef487",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "wordCounts.write.mode('overwrite').parquet('/opt/warehouse/wordcounts.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973833a6-4b07-4aff-af97-5ff6df419b7d",
   "metadata": {},
   "source": [
    "## Read back Parquet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b59f6a-6d4e-43c3-8abd-7a496defb0cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       word|count|\n",
      "+-----------+-----+\n",
      "|     online|    4|\n",
      "|PERMISSION.|    7|\n",
      "|       some|   26|\n",
      "|  disgrace,|    1|\n",
      "|       hope|    8|\n",
      "|      still|    7|\n",
      "|         By|   24|\n",
      "| misplaced;|    1|\n",
      "|      those|    8|\n",
      "|    knight,|    1|\n",
      "| FREDERICK.|   20|\n",
      "|  wrestler?|    1|\n",
      "|    embrace|    1|\n",
      "|        art|   21|\n",
      "|      burs,|    1|\n",
      "| likelihood|    1|\n",
      "|     travel|    3|\n",
      "|assailants.|    1|\n",
      "|      cold,|    1|\n",
      "|    blossom|    1|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "wc2 = spark_session.read.parquet('/opt/warehouse/wordcounts.parquet/')\n",
    "wc2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5569fdc2-9ca2-4697-b489-f9c0f29e6b0f",
   "metadata": {},
   "source": [
    "### Enable SQL-querying\n",
    "Create a temp-view from wc2 with name \"wordcounts\" so we can reference that as a table name in subsequent SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cbde442-e27f-4120-8a3d-2b646fe02fad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|   ROSALIND.|  201|\n",
      "|    ORLANDO.|  120|\n",
      "|      CELIA.|  109|\n",
      "|     Project|   78|\n",
      "| TOUCHSTONE.|   74|\n",
      "|       would|   68|\n",
      "|       shall|   61|\n",
      "|     JAQUES.|   57|\n",
      "|Gutenberg-tm|   53|\n",
      "|       Enter|   51|\n",
      "|       which|   50|\n",
      "|     OLIVER.|   37|\n",
      "|      should|   35|\n",
      "|       there|   35|\n",
      "|       these|   32|\n",
      "|     SENIOR.|   32|\n",
      "|       their|   31|\n",
      "|  electronic|   27|\n",
      "|      cannot|   27|\n",
      "|      Exeunt|   27|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/16 21:27:39 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "23/04/16 21:27:39 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:978)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "wc2.createOrReplaceTempView(\"wordcounts\")\n",
    "\n",
    "ans = spark_session.sql(\"SELECT * FROM wordcounts WHERE LEN(word) > 4 ORDER BY count DESC\")\n",
    "ans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56f92ac3-5ec9-455b-b9a7-6cfe45f498bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ans.limit(10).write.json('/opt/warehouse/answer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a6ca5ed-f6c1-4cf6-973f-4b8fb8717d55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/16 21:00:50 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "23/04/16 21:00:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:978)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "list_of_dicts = ans.limit(10).rdd.map(lambda row: row.asDict()).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d476c5c-cb5f-48ea-89c2-3214150680e1",
   "metadata": {},
   "source": [
    "# Close Session\n",
    "This shuts down the executors running on the workers and relinquishes cluster resources associated with this app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5426c84e-d519-4e83-83fa-502cdaceb44c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b59cf-52b4-4732-8e2f-3c950527780d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
